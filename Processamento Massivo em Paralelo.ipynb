{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/castrokelly/MBA/blob/main/Processamento%20Massivo%20em%20Paralelo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flekT6GFDN6m"
      },
      "source": [
        "# <span style=\"color:blue\">MBA em Ciência de Dados</span>\n",
        "# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
        "\n",
        "## <span style=\"color:blue\">Avaliação Final - Notebook </span>\n",
        "\n",
        "**Material Produzido por:**<br>\n",
        ">**Profa. Dra. Cristina Dutra de Aguiar**<br>\n",
        "\n",
        "\n",
        "**CEMEAI - ICMC/USP São Carlos**\n",
        "\n",
        "Este *notebook* deve conter as respostas para as consultas analíticas solicitadas nas Questões 7, 8, 9 e 10. É possível especificar as consultas analíticas usando Pandas, o método spark.sql() ou os métodos do módulo pyspark.sql. As consultas devem ser especificadas na seção 6, na célula indicada para resposta.\n",
        "\n",
        "**IMPORTANTE**\n",
        "\n",
        "- **As respostas para as Questões 7, 8, 9 e 10 somente serão consideradas se forem especificadas no *notebook*. Portanto, independentemente da alternativa estar certa ou errada, se a consulta analítica correspondente não for especificada, a alternativa será considerada errada.**\n",
        "\n",
        "- **Caso uma alternativa esteja certa, porém a especificação da consulta analítica correspondente estiver errada no *notebook*, a alternativa será considerada errada.**\n",
        "\n",
        "\n",
        "O *notebook* contém a constelação de fatos da BI Solutions que deve ser utilizada para responder às questões e também todas as bibliotecas, bases de dados, inicializações, instalações, importações, geração de dataFrames, geração de visões temporárias e conversão dos tipos de dados necessárias para a realização da questão. Portanto, o *notebook* está preparado para ser executado usando Pandas, o método spark.sql() e os métodos do módulo pyspark.sql.\n",
        "\n",
        "O uso do *framework* Spark requer diversas configurações no ambiente de desenvolvimento para executar o *notebook*. Dado que tal complexidade foge do escopo de nossa disciplina, recomenda-se que o *notebook* seja executado na plataforma de desenvolvimento COLAB. O uso do COLAB  proporciona um ambiente de desenvolvimento pré-configurado e remove a complexidade de instalação e configuração de pacotes e *frameworks* que são utilizados na disciplina.\n",
        "\n",
        "**INSTRUÇÕES DE ENTREGA**\n",
        "\n",
        "**O que deve ser entregue:**\n",
        "- **O notebook com as respostas no formato .ipynb**\n",
        "- **O notebook com as respostas no formato .pdf**\n",
        "\n",
        "**Ambos arquivos devem ser nomeados usando o primeiro nome e o último sobrenome do aluno. Por exemplo: CristinaAguiar.ipynb e CristinaAguiar.pdf.**\n",
        "\n",
        "Boa avaliação!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o3dN_WLQcyD"
      },
      "source": [
        "#1 Constelação de Fatos da BI Solutions\n",
        "\n",
        "A aplicação de *data warehousing* da BI Solutions utiliza como base uma constelação de fatos, conforme descrita a seguir.\n",
        "\n",
        "**Tabelas de dimensão**\n",
        "\n",
        "- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n",
        "- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n",
        "- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n",
        "- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n",
        "- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n",
        "\n",
        "**Tabelas de fatos**\n",
        "- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n",
        "- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGeh8KdXwVCQ"
      },
      "source": [
        "#2 Obtenção dos Dados da BI Solutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCCNC64AzBG0"
      },
      "source": [
        "## 2.1 Baixando o Módulo wget\n",
        "\n",
        "Para baixar os dados referentes ao esquema relacional da constelação de fatos da BI Solutions, é utilizado o módulo  **wget**. O comando a seguir realiza a instalação desse módulo. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0Eao1K0EYG"
      },
      "source": [
        "#instalando o módulo wget\n",
        "%%capture\n",
        "!pip install -q wget\n",
        "!mkdir data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j56pVJ2hZ2i5"
      },
      "source": [
        "## 2.2 Obtenção dos Dados das Tabelas de Dimensão\n",
        "\n",
        "Os comandos a seguir baixam os dados que povoam as tabelas de dimensão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46QzTpLJwfkW",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e92f1965-8b35-43e5-e947-b8e2c78e7cf5"
      },
      "source": [
        "#baixando os dados das tabelas de dimensão\n",
        "import wget\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\"\n",
        "wget.download(url, \"data/data.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\"\n",
        "wget.download(url, \"data/funcionario.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\"\n",
        "wget.download(url, \"data/equipe.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\"\n",
        "wget.download(url, \"data/cargo.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\"\n",
        "wget.download(url, \"data/cliente.csv\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/cliente.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o-dC7feszRc"
      },
      "source": [
        "## 2.3 Obtenção dos Dados Tabelas de Fatos\n",
        "\n",
        "Os comandos a seguir baixam os dados que povoam as tabelas de fatos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWM-CUFgBl_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5994697f-08a0-43b7-99b0-3686d76b3fb4"
      },
      "source": [
        "#baixando os dados das tabelas de fatos\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\"\n",
        "wget.download(url, \"data/pagamento.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n",
        "wget.download(url, \"data/negociacao.csv\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/negociacao.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO16-7-jOioq"
      },
      "source": [
        "# 3 Apache Spark Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVEgY9qKflBV"
      },
      "source": [
        "## 3.1 Instalação\n",
        "\n",
        "Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaM-OnIjgLS2"
      },
      "source": [
        "Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXls3bfoglKW"
      },
      "source": [
        "#instalando Java Runtime Environment (JRE) versão 8\n",
        "%%capture\n",
        "!apt-get remove openjdk*\n",
        "!apt-get update --fix-missing\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BQzZfDYhb4j"
      },
      "source": [
        "Na sequência, é feito o *download* do Apache Spark versão 3.0.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a_Yv59zg3gm"
      },
      "source": [
        "#baixando Apache Spark versão 3.0.0\n",
        "%%capture\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RETWX6wqhkLf"
      },
      "source": [
        "Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpR7NwOh2EB"
      },
      "source": [
        "import os\n",
        "#configurando a variável de ambiente JAVA_HOME\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#configurando a variável de ambiente SPARK_HOME\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql0z7Ro1iHQb"
      },
      "source": [
        "Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n",
        "\n",
        "> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark.\n",
        "\n",
        "> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSYOwKljPf5"
      },
      "source": [
        "%%capture\n",
        "#instalando o pacote findspark\n",
        "!pip install -q findspark==1.4.2\n",
        "#instalando o pacote pyspark\n",
        "!pip install -q pyspark==3.0.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAaLyjPzmIwZ"
      },
      "source": [
        "## 3.2 Conexão\n",
        "\n",
        "PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo.\n",
        "\n",
        "Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zm1pBTEmjp4"
      },
      "source": [
        "#importando o módulo findspark\n",
        "import findspark\n",
        "#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n",
        "findspark.init()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDqfefF7YUab"
      },
      "source": [
        "Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível iniciar o uso do Spark na aplicação de `data warehousing`. Para tanto, é necessário importar o comando `SparkSession` do módulo `pyspark.sql`. São utilizados os seguintes conceitos: <br>\n",
        "\n",
        "- `SparkSession`: permite a criação de `DataFrames`. Como resultado, as tabelas relacionais podem ser manipuladas por meio de `DataFrames` e é possível realizar consultas OLAP por meio de comandos SQL. <br>\n",
        "- `builder`: cria uma instância de SparkSession. <br>\n",
        "- `appName`: define um nome para a aplicação, o qual pode ser visto na interface de usuário web do Spark. <br>\n",
        "- `master`: define onde está o nó mestre do *cluster*. Como a aplicação é executada localmente e não em um *cluster*, indica-se isso pela *string* `local` seguida do parâmetro `[*]`. Ou seja, define-se que apenas núcleos locais são utilizados.\n",
        "- `getOrCreate`: cria uma SparkSession. Caso ela já exista, retorna a instância existente.\n",
        "\n",
        "\n",
        "**Observação**: A lista completa de todos os parâmetros que podem ser utilizados na inicialização do *cluster* pode ser encontrada neste [link](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TxljJ_cwBCy"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IpYfoG_kx_8"
      },
      "source": [
        "# 4 Geração dos DataFrames em Pandas da BI Solutions\n",
        "\n",
        "Nesta seção são gerados os DataFrames em Pandas. Atenção aos nomes desses DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9arYf_PHlJCR"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw4NfyDZ--6z"
      },
      "source": [
        "cargoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv')\n",
        "clientePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv')\n",
        "dataPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv')\n",
        "equipePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv')\n",
        "funcionarioPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv')\n",
        "negociacaoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv')\n",
        "pagamentoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qL9SiR_pQE2"
      },
      "source": [
        "# 5 Geração dos DataFrames em Spark da BI Solutions\n",
        "\n",
        "Nesta seção são gerados dos DataFrames em Spark. Atenção aos nomes desses DataFrames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRVoz-SGt87W"
      },
      "source": [
        "## 5.1 Criação dos DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "FNR-3dV6oYk4"
      },
      "source": [
        "#criando os DataFrames em Spark\n",
        "cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n",
        "cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n",
        "data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\")\n",
        "equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n",
        "funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n",
        "negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n",
        "pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrch9vLgjl_H"
      },
      "source": [
        "## 5.2 Atualização dos Tipos de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A_ot2pOjsWB"
      },
      "source": [
        "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado inteiro. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmCV6Mur__z6"
      },
      "source": [
        "# identificando quais colunas de quais DataFrames devem ser do tipo de dado inteiro\n",
        "colunas_cargo = [\"cargoPK\"]\n",
        "colunas_cliente = [\"clientePK\"]\n",
        "colunas_data = [\"dataPK\", \"dataDia\", \"dataMes\", \"dataBimestre\", \"dataTrimestre\", \"dataSemestre\", \"dataAno\"]\n",
        "colunas_equipe = [\"equipePK\"]\n",
        "colunas_funcionario = [\"funcPK\", \"funcDiaNascimento\", \"funcMesNascimento\", \"funcAnoNascimento\"]\n",
        "colunas_negociacao = [\"equipePK\", \"clientePK\", \"dataPK\", \"quantidadeNegociacoes\"]\n",
        "colunas_pagamento = [\"funcPK\", \"equipePK\", \"dataPK\", \"cargoPK\", \"quantidadeLancamentos\"]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPNnDJcG9R5H"
      },
      "source": [
        "# importando o tipo de dado desejado\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "\n",
        "# atualizando o tipo de dado das colunas especificadas\n",
        "# substituindo as colunas já existentes\n",
        "\n",
        "for coluna in colunas_cargo:\n",
        "  cargo = cargo.withColumn(coluna, cargo[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_cliente:\n",
        "  cliente = cliente.withColumn(coluna, cliente[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_data:\n",
        "  data = data.withColumn(coluna, data[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_equipe:\n",
        "  equipe = equipe.withColumn(coluna, equipe[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_funcionario:\n",
        "  funcionario = funcionario.withColumn(coluna, funcionario[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_negociacao:\n",
        "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_pagamento:\n",
        "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(IntegerType()))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0dX_7U_AzIY"
      },
      "source": [
        "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado número de ponto flutuante. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBcQ7Ep7AWqN"
      },
      "source": [
        "# identificando quais colunas de quais DataFrames devem ser do tipo de dado número de ponto flutuante\n",
        "colunas_negociacao = [\"receita\"]\n",
        "colunas_pagamento = [\"salario\"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcfvkIK1BRSp"
      },
      "source": [
        "# importando o tipo de dado desejado\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "\n",
        "# atualizando o tipo de dado das colunas especificadas\n",
        "# substituindo as colunas já existentes\n",
        "\n",
        "for coluna in colunas_negociacao:\n",
        "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(FloatType()))\n",
        "\n",
        "for coluna in colunas_pagamento:\n",
        "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(FloatType()))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91nwfsV3_rKs"
      },
      "source": [
        "# importando funções adicionais\n",
        "from pyspark.sql.functions import round, desc"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wN5iOGKwnHG"
      },
      "source": [
        "## 5.3 Criação de Visões Temporárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJsqRI3TwsjS"
      },
      "source": [
        "#criando as visões temporárias\n",
        "cargo.createOrReplaceTempView(\"cargo\")\n",
        "cliente.createOrReplaceTempView(\"cliente\")\n",
        "data.createOrReplaceTempView(\"data\")\n",
        "equipe.createOrReplaceTempView(\"equipe\")\n",
        "funcionario.createOrReplaceTempView(\"funcionario\")\n",
        "negociacao.createOrReplaceTempView(\"negociacao\")\n",
        "pagamento.createOrReplaceTempView(\"pagamento\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Respostas"
      ],
      "metadata": {
        "id": "2X_PNhxFCKh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lembre-se que é possível especificar as consultas analíticas usando Pandas, o método spark.sql() ou os métodos do módulo pyspark.sql."
      ],
      "metadata": {
        "id": "vqsvtdy4CdVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Especificação da Consulta Analítica da Questão 7"
      ],
      "metadata": {
        "id": "ZQcs4bV9uRf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, round, col, desc\n",
        "\n",
        "# Inicializar SparkSession\n",
        "spark = SparkSession.builder.appName(\"Questao7\").getOrCreate()\n",
        "\n",
        "resultado = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        f.funcSexo AS SEXO,            -- Seleciona o campo funcSexo como SEXO\n",
        "        f.funcAnoNascimento AS ANO,   -- Seleciona o campo funcAnoNascimento como ANO\n",
        "        ROUND(AVG(p.salario), 2) AS MEDIASALARIO  -- Calcula a média dos salários e arredonda para 2 casas decimais\n",
        "    FROM\n",
        "        pagamento p                   -- Tabela de fatos \"pagamento\"\n",
        "    JOIN\n",
        "        funcionario f                 -- Junta com a tabela de dimensão \"funcionario\"\n",
        "    ON\n",
        "        p.funcPK = f.funcPK           -- Junta as tabelas pela chave primária do funcionário\n",
        "    WHERE\n",
        "        f.funcRegiaoNome = 'NORDESTE' -- Filtra apenas funcionários que residem na região NORDESTE\n",
        "    GROUP BY\n",
        "        CUBE(f.funcSexo, f.funcAnoNascimento)  -- Gera todas as combinações de agregações\n",
        "    ORDER BY\n",
        "        SEXO ASC NULLS LAST,          -- Ordena por SEXO, com NULLs no final\n",
        "        ANO ASC NULLS LAST,           -- Ordena por ANO, com NULLs no final\n",
        "        MEDIASALARIO ASC              -- Ordena por média salarial em ordem crescente\n",
        "\"\"\")\n",
        "\n",
        "# Exibir os resultados\n",
        "resultado.show(50)\n"
      ],
      "metadata": {
        "id": "CDzO2MJouUWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d7f8b3-34ab-4d93-8be6-8c9a03dc60c4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|SEXO| ANO|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|   F|1953|    11842.55|\n",
            "|   F|1990|     5935.93|\n",
            "|   F|null|     7043.42|\n",
            "|   M|1951|     4116.83|\n",
            "|   M|1952|     3635.55|\n",
            "|   M|1965|     1919.34|\n",
            "|   M|1966|    14995.02|\n",
            "|   M|1967|     6686.45|\n",
            "|   M|1978|     2581.24|\n",
            "|   M|1979|     6300.52|\n",
            "|   M|1980|     2085.85|\n",
            "|   M|1990|     8742.07|\n",
            "|   M|null|     7183.82|\n",
            "|null|1951|     4116.83|\n",
            "|null|1952|     3635.55|\n",
            "|null|1953|    11842.55|\n",
            "|null|1965|     1919.34|\n",
            "|null|1966|    14995.02|\n",
            "|null|1967|     6686.45|\n",
            "|null|1978|     2581.24|\n",
            "|null|1979|     6300.52|\n",
            "|null|1980|     2085.85|\n",
            "|null|1990|     7782.07|\n",
            "|null|null|     7151.73|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Média salarial geral\n",
        "media_geral = resultado.filter(\"Sexo IS NULL AND Ano IS NULL\")\n",
        "media_geral.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G5EXLZUtQe3",
        "outputId": "8f665206-31fa-45d1-9fef-d0fb640ad8a8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|SEXO| ANO|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|null|null|     7151.73|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Maior média salarial (sexo M, ano 1966)\n",
        "maior_media = resultado.filter(\"Sexo = 'M' AND Ano = 1966\")\n",
        "maior_media.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmHB8h19tUxs",
        "outputId": "6a0d4925-bce4-454c-df63-3b8931a1eef6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|Sexo| Ano|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|   M|1966|    14995.02|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menor média salarial (sexo M, ano 1965)\n",
        "menor_media = resultado.filter(\"Sexo = 'M' AND Ano = 1965\")\n",
        "menor_media.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw0rj87Gtd8V",
        "outputId": "e3734755-d683-428f-e054-987a41cdf04f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|Sexo| Ano|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|   M|1965|     1919.34|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Média salarial das funcionárias de todos os anos (quando ANO é nulo)\n",
        "media_funcionarias = resultado.filter(\"Sexo = 'F' AND Ano IS NULL\")\n",
        "media_funcionarias.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x2t_sAUvPz3",
        "outputId": "b4a2066e-2f98-4f9b-e59f-914362ff46e3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|Sexo| Ano|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|   F|null|     7043.42|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Média salarial dos funcionários masculinos de todos os anos (quando ANO é nulo)\n",
        "media_funcionarios = resultado.filter(\"Sexo = 'M' AND Ano IS NULL\")\n",
        "media_funcionarios.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0eDnlAAvSM3",
        "outputId": "dd33988e-46f3-4b0f-f893-299e915d4784"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|Sexo| Ano|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|   M|null|     7183.82|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Média salarial para o ano de 1990, independentemente do sexo\n",
        "media_1990 = resultado.filter(\"Ano = 1990\")\n",
        "media_1990.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2diMnLWvU9r",
        "outputId": "19542ae8-1081-4d02-d616-ee9572384280"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|Sexo| Ano|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|null|1990|     7782.07|\n",
            "|   F|1990|     5935.93|\n",
            "|   M|1990|     8742.07|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consulta Analítica Realizada:\n",
        "\n",
        "A consulta calculou as médias salariais dos funcionários que residem na região Nordeste, agrupando por `sexo` e `ano de nascimento`.\n",
        "\n",
        "Valores nulos foram incluídos no agrupamento para contemplar casos onde sexo ou ano de nascimento não estão disponíveis.\n",
        "\n",
        "A média salarial foi arredondada para duas casas decimais.\n",
        "\n",
        "* Resultados Obtidos:\n",
        "\n",
        "Foram retornadas 24 linhas.\n",
        "\n",
        "Os anos de nascimento variam de 1951 a 1990.\n",
        "\n",
        "Existem valores nulos tanto no campo sexo quanto no campo ano de nascimento."
      ],
      "metadata": {
        "id": "ogPsn4SPwDrc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcSouuCk6N0-"
      },
      "source": [
        "## Especificação da Consulta Analítica da Questão 8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar as colunas disponíveis na visão temporária `cliente`\n",
        "print(\"Colunas disponíveis na visão cliente:\")\n",
        "spark.sql(\"SELECT * FROM cliente\").printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3ZUkc7B4Ys5",
        "outputId": "1810bd8a-2616-4f8b-f0c0-829ae4d86e3d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas disponíveis na visão cliente:\n",
            "root\n",
            " |-- clientePK: integer (nullable = true)\n",
            " |-- clienteNomeFantasia: string (nullable = true)\n",
            " |-- clienteSetor: string (nullable = true)\n",
            " |-- clienteCidade: string (nullable = true)\n",
            " |-- clienteEstadoNome: string (nullable = true)\n",
            " |-- clienteEstadoSigla: string (nullable = true)\n",
            " |-- clienteRegiaoNome: string (nullable = true)\n",
            " |-- clienteRegiaoSigla: string (nullable = true)\n",
            " |-- clientePaisNome: string (nullable = true)\n",
            " |-- clientePaisSigla: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, round\n",
        "\n",
        "# Inicializar SparkSession\n",
        "spark = SparkSession.builder.appName(\"Questao8\").getOrCreate()\n",
        "\n",
        "# 1. Consulta intermediária\n",
        "consulta_intermediaria = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        d.dataAno AS ANO,                      -- Seleciona o ano da data\n",
        "        cl.clienteSetor AS SETOR,              -- Seleciona o setor do cliente\n",
        "        SUM(n.receita) AS SOMARECEITA          -- Soma a receita\n",
        "    FROM\n",
        "        negociacao n\n",
        "    JOIN\n",
        "        cliente cl ON cl.clientePK = n.clientePK -- Junção com a tabela cliente\n",
        "    JOIN\n",
        "        data d ON n.dataPK = d.dataPK          -- Junção com a tabela data\n",
        "    WHERE\n",
        "        d.dataAno IN (2019, 2020)              -- Filtra os anos de interesse\n",
        "        AND cl.clienteEstadoNome = 'SAO PAULO' -- Filtra clientes localizados no Estado de São Paulo\n",
        "    GROUP BY\n",
        "        d.dataAno, cl.clienteSetor             -- Agrupa por ano e setor\n",
        "    ORDER BY\n",
        "        d.dataAno ASC, cl.clienteSetor ASC     -- Ordena pelo ano e setor\n",
        "\"\"\")\n",
        "\n",
        "# Print da consulta intermediária\n",
        "print(\"Consulta intermediária após WHERE e GROUP BY:\")\n",
        "consulta_intermediaria.show()\n"
      ],
      "metadata": {
        "id": "4OjKqHvginUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4404c48d-780d-4b33-eae2-eddcd0397592"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta intermediária após WHERE e GROUP BY:\n",
            "+----+-------------------+------------------+\n",
            "| ANO|              SETOR|       SOMARECEITA|\n",
            "+----+-------------------+------------------+\n",
            "|2019|BEBIDAS E ALIMENTOS|        3995904.25|\n",
            "|2019|            CREDITO|1347436.2541503906|\n",
            "|2019|              SAUDE|     1914114.15625|\n",
            "|2019|         TECNOLOGIA| 5778946.271240234|\n",
            "|2019|          VESTUARIO|1680762.3587646484|\n",
            "|2020|BEBIDAS E ALIMENTOS| 4264757.815917969|\n",
            "|2020|            CREDITO|1393427.5520019531|\n",
            "|2020|              SAUDE| 1292370.146118164|\n",
            "|2020|         TECNOLOGIA| 3567224.553955078|\n",
            "|2020|          VESTUARIO|2782399.5897216797|\n",
            "+----+-------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consulta Intermediária:\n",
        "\n",
        "Os dados foram filtrados inicialmente pelos anos de 2019 e 2020 (`d.dataAno IN (2019, 2020)`) e pelos clientes localizados no estado de São Paulo (`cl.clienteEstadoNome = 'SAO PAULO'`).\n",
        "\n",
        "Após aplicar os filtros, a soma das receitas por ano e setor foi calculada usando a função `SUM(n.receita)` e agrupada por `ANO` e `SETOR`.\n",
        "\n",
        "O resultado da consulta intermediária incluiu as colunas `ANO`, `SETOR`, e `SOMARECEITA`."
      ],
      "metadata": {
        "id": "CYGu0p2V6Vky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicação do filtro (HAVING) separadamente\n",
        "resultado_final = (\n",
        "    consulta_intermediaria\n",
        "    .filter(col(\"SOMARECEITA\") > 3900000)  # Filtro de receita superior a R$ 3.900.000\n",
        "    .select(\n",
        "        col(\"ANO\"),\n",
        "        col(\"SETOR\"),\n",
        "        round(col(\"SOMARECEITA\"), 2).alias(\"TOTALRECEITA\")  # Arredondamento para duas casas decimais\n",
        "    )\n",
        "    .orderBy(\"ANO\", \"SETOR\")  # Ordenação final\n",
        ")\n",
        "\n",
        "# Print do resultado final\n",
        "print(\"Resultado final após aplicação do filtro HAVING:\")\n",
        "resultado_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YrBqiv20ToW",
        "outputId": "325b5e11-7c3a-4ff6-a74a-47d468ea9ee2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado final após aplicação do filtro HAVING:\n",
            "+----+-------------------+------------+\n",
            "| ANO|              SETOR|TOTALRECEITA|\n",
            "+----+-------------------+------------+\n",
            "|2019|BEBIDAS E ALIMENTOS|  3995904.25|\n",
            "|2019|         TECNOLOGIA|  5778946.27|\n",
            "|2020|BEBIDAS E ALIMENTOS|  4264757.82|\n",
            "+----+-------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicação do Filtro (`HAVING`):\n",
        "\n",
        "Após a consulta intermediária, foi aplicado o filtro para considerar apenas os setores com receitas superiores a R$ 3.900.000,00 `(SOMARECEITA > 3900000`).\n",
        "O resultado final foi refinado para exibir as colunas `ANO`, `SETOR`, e `TOTALRECEITA`, onde a soma das receitas foi arredondada para duas casas decimais.\n",
        "\n",
        "Ordenação dos Resultados:\n",
        "\n",
        "O resultado final foi ordenado pelas colunas `ANO` e `SETOR`.\n",
        "\n",
        "Resultados:\n",
        "\n",
        "3 linhas\n",
        "\n",
        "Para o ano de 2019, os setores de maior receita foram BEBIDAS E ALIMENTOS e TECNOLOGIA, com receitas de `R$ 3.959.904,25` e `R$ 5.778.946,27`, respectivamente.\n",
        "\n",
        "Para o ano de 2020, o setor de maior receita foi BEBIDAS E ALIMENTOS, com R$ 4.264.757,82."
      ],
      "metadata": {
        "id": "hUWVeWNQ6wp1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpFnRTk369Q5"
      },
      "source": [
        "## Especificação da Consulta Analítica da Questão 9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar as colunas disponíveis na visão temporária 'equipe'\n",
        "print(\"Colunas disponíveis na visão equipe:\")\n",
        "spark.sql(\"SELECT * FROM equipe\").printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZbxBOgE8W6E",
        "outputId": "f65fdc9d-a78a-4e3e-9e61-5a2f35fc7243"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas disponíveis na visão equipe:\n",
            "root\n",
            " |-- equipePK: integer (nullable = true)\n",
            " |-- equipeNome: string (nullable = true)\n",
            " |-- filialNome: string (nullable = true)\n",
            " |-- filialCidade: string (nullable = true)\n",
            " |-- filialEstadoNome: string (nullable = true)\n",
            " |-- filialEstadoSigla: string (nullable = true)\n",
            " |-- filialRegiaoNome: string (nullable = true)\n",
            " |-- filialRegiaoSigla: string (nullable = true)\n",
            " |-- filialPaisNome: string (nullable = true)\n",
            " |-- filialPaisSigla: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, round, when\n",
        "\n",
        "# Inicializar SparkSession\n",
        "spark = SparkSession.builder.appName(\"Questao9\").getOrCreate()\n",
        "\n",
        "# Consulta intermediária: calcular receitas por equipe e ano\n",
        "consulta_receitas = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        e.equipeNome AS EQUIPENOME,               -- Nome da equipe\n",
        "        e.filialNome AS EQUIPEFILIAL,             -- Nome da filial da equipe (corrigido)\n",
        "        d.dataAno AS ANO,                         -- Ano\n",
        "        SUM(n.receita) AS TOTALRECEITA            -- Soma das receitas\n",
        "    FROM\n",
        "        negociacao n\n",
        "    JOIN\n",
        "        equipe e ON n.equipePK = e.equipePK       -- Junção com a tabela equipe\n",
        "    JOIN\n",
        "        data d ON n.dataPK = d.dataPK             -- Junção com a tabela data\n",
        "    WHERE\n",
        "        d.dataAno IN (2019, 2020)                 -- Filtrar anos de 2019 e 2020\n",
        "    GROUP BY\n",
        "        e.equipeNome, e.filialNome, d.dataAno     -- Agrupar por equipe, filial, e ano\n",
        "    ORDER BY\n",
        "        e.equipeNome, e.filialNome, d.dataAno     -- Ordenar para facilitar visualização\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar a consulta intermediária\n",
        "print(\"Consulta de receita por equipe e ano:\")\n",
        "consulta_receitas.show()"
      ],
      "metadata": {
        "id": "m98NQbOiio-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090e2124-9439-4070-a447-539229970349"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta de receita por equipe e ano:\n",
            "+--------------+--------------------+----+--------------------+\n",
            "|    EQUIPENOME|        EQUIPEFILIAL| ANO|        TOTALRECEITA|\n",
            "+--------------+--------------------+----+--------------------+\n",
            "| APP - DESKTOP|RIO DE JANEIRO - ...|2019|  2417619.8994140625|\n",
            "| APP - DESKTOP|RIO DE JANEIRO - ...|2020|  2290441.2983398438|\n",
            "| APP - DESKTOP|SAO PAULO - AV. P...|2019|  2009714.0009765625|\n",
            "| APP - DESKTOP|SAO PAULO - AV. P...|2020|  2146181.2392578125|\n",
            "|  APP - MOBILE|CAMPO GRANDE - CE...|2019|  1280914.8969726562|\n",
            "|  APP - MOBILE|CAMPO GRANDE - CE...|2020|  1347929.7026367188|\n",
            "|  APP - MOBILE|RIO DE JANEIRO - ...|2019|   1645371.005859375|\n",
            "|  APP - MOBILE|RIO DE JANEIRO - ...|2020|  1289304.9975585938|\n",
            "|  APP - MOBILE|SAO PAULO - AV. P...|2019|  1779690.5947265625|\n",
            "|  APP - MOBILE|SAO PAULO - AV. P...|2020|  1243670.5493164062|\n",
            "|BI & ANALYTICS|     RECIFE - CENTRO|2019|1.2310646513671875E7|\n",
            "|BI & ANALYTICS|     RECIFE - CENTRO|2020|    8791572.87109375|\n",
            "|BI & ANALYTICS|SAO PAULO - AV. P...|2019|1.1267226419921875E7|\n",
            "|BI & ANALYTICS|SAO PAULO - AV. P...|2020|   1.0730773515625E7|\n",
            "|           WEB|CAMPO GRANDE - CE...|2019|   956287.2945556641|\n",
            "|           WEB|CAMPO GRANDE - CE...|2020|   1017644.055480957|\n",
            "|           WEB|RIO DE JANEIRO - ...|2019|  1037993.7048339844|\n",
            "|           WEB|RIO DE JANEIRO - ...|2020|   612673.8998413086|\n",
            "|           WEB|SAO PAULO - AV. P...|2019|   647854.0006103516|\n",
            "|           WEB|SAO PAULO - AV. P...|2020|   751983.7419433594|\n",
            "+--------------+--------------------+----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar a visão temporária 'consulta_receitas'\n",
        "consulta_receitas.createOrReplaceTempView(\"consulta_receitas\")"
      ],
      "metadata": {
        "id": "Hl-3UZaR9m1f"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular receitas de 2019, 2020, e a diferença entre os dois anos\n",
        "resultado_final = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        r2019.EQUIPENOME,\n",
        "        r2019.EQUIPEFILIAL,\n",
        "        COALESCE(r2019.TOTALRECEITA, 0) AS RECEITA2019,\n",
        "        COALESCE(r2020.TOTALRECEITA, 0) AS RECEITA2020,\n",
        "        COALESCE(r2020.TOTALRECEITA, 0) - COALESCE(r2019.TOTALRECEITA, 0) AS DIFERENCA\n",
        "    FROM\n",
        "        (SELECT EQUIPENOME, EQUIPEFILIAL, TOTALRECEITA\n",
        "         FROM consulta_receitas WHERE ANO = 2019) r2019\n",
        "    FULL OUTER JOIN\n",
        "        (SELECT EQUIPENOME, EQUIPEFILIAL, TOTALRECEITA\n",
        "         FROM consulta_receitas WHERE ANO = 2020) r2020\n",
        "    ON\n",
        "        r2019.EQUIPENOME = r2020.EQUIPENOME AND r2019.EQUIPEFILIAL = r2020.EQUIPEFILIAL\n",
        "    ORDER BY\n",
        "        DIFERENCA DESC, EQUIPENOME, EQUIPEFILIAL\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar o resultado final\n",
        "print(\"Resultado final:\")\n",
        "resultado_final.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25uerAa_84Ji",
        "outputId": "fcb121c3-b84d-44c0-f07d-02334eccba26"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado final:\n",
            "+--------------+--------------------+--------------------+------------------+-------------------+\n",
            "|    EQUIPENOME|        EQUIPEFILIAL|         RECEITA2019|       RECEITA2020|          DIFERENCA|\n",
            "+--------------+--------------------+--------------------+------------------+-------------------+\n",
            "| APP - DESKTOP|SAO PAULO - AV. P...|  2009714.0009765625|2146181.2392578125|    136467.23828125|\n",
            "|           WEB|SAO PAULO - AV. P...|   647854.0006103516| 751983.7419433594| 104129.74133300781|\n",
            "|  APP - MOBILE|CAMPO GRANDE - CE...|  1280914.8969726562|1347929.7026367188|   67014.8056640625|\n",
            "|           WEB|CAMPO GRANDE - CE...|   956287.2945556641| 1017644.055480957|  61356.76092529297|\n",
            "| APP - DESKTOP|RIO DE JANEIRO - ...|  2417619.8994140625|2290441.2983398438|-127178.60107421875|\n",
            "|  APP - MOBILE|RIO DE JANEIRO - ...|   1645371.005859375|1289304.9975585938|-356066.00830078125|\n",
            "|           WEB|RIO DE JANEIRO - ...|  1037993.7048339844| 612673.8998413086| -425319.8049926758|\n",
            "|  APP - MOBILE|SAO PAULO - AV. P...|  1779690.5947265625|1243670.5493164062| -536020.0454101562|\n",
            "|BI & ANALYTICS|SAO PAULO - AV. P...|1.1267226419921875E7| 1.0730773515625E7|  -536452.904296875|\n",
            "|BI & ANALYTICS|     RECIFE - CENTRO|1.2310646513671875E7|  8791572.87109375| -3519073.642578125|\n",
            "+--------------+--------------------+--------------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com base nos dados obtidos:\n",
        "\n",
        "* A equipe APP - DESKTOP da filial SAO PAULO - AV. PAULISTA apresentou uma diferença positiva de 13.647,23, indicando que a receita de 2020 foi maior que a de 2019.\n",
        "\n",
        "* A equipe WEB da filial SAO PAULO - AV. PAULISTA teve uma diferença positiva de 104.219,13.\n",
        "\n",
        "* A equipe BI & ANALYTICS da filial RECIFE - CENTRO apresentou a maior diferença negativa, com -351.917,62, indicando que sua receita caiu drasticamente de 2019 para 2020.\n",
        "\n",
        "* As equipes APP - MOBILE e WEB em várias filiais também apresentaram receitas com diferenças variadas."
      ],
      "metadata": {
        "id": "ZK8VRt31-tuN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCpy46lL8Xdn"
      },
      "source": [
        "## Especificação da Consulta Analítica da Questão 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Esquema da visão temporária 'funcionario':\")\n",
        "spark.sql(\"SELECT * FROM funcionario\").printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABS1hGjz_hm6",
        "outputId": "97808640-f35a-459d-cd7f-ce0c177986f2"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esquema da visão temporária 'funcionario':\n",
            "root\n",
            " |-- funcPK: integer (nullable = true)\n",
            " |-- funcMatricula: string (nullable = true)\n",
            " |-- funcNome: string (nullable = true)\n",
            " |-- funcSexo: string (nullable = true)\n",
            " |-- funcDataNascimento: string (nullable = true)\n",
            " |-- funcDiaNascimento: integer (nullable = true)\n",
            " |-- funcMesNascimento: integer (nullable = true)\n",
            " |-- funcAnoNascimento: integer (nullable = true)\n",
            " |-- funcCidade: string (nullable = true)\n",
            " |-- funcEstadoNome: string (nullable = true)\n",
            " |-- funcEstadoSigla: string (nullable = true)\n",
            " |-- funcRegiaoNome: string (nullable = true)\n",
            " |-- funcRegiaoSigla: string (nullable = true)\n",
            " |-- funcPaisNome: string (nullable = true)\n",
            " |-- funcPaisSigla: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, round\n",
        "\n",
        "# Inicializar SparkSession\n",
        "spark = SparkSession.builder.appName(\"Questao10\").getOrCreate()\n",
        "\n",
        "# Consulta para calcular os gastos em salários\n",
        "consulta_salarios = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        f.funcEstadoNome AS ESTADO,             -- Nome do estado\n",
        "        f.funcRegiaoNome AS REGIAO,             -- Nome da região\n",
        "        ROUND(SUM(p.salario), 2) AS TOTALSALARIOS -- Soma dos salários\n",
        "    FROM\n",
        "        pagamento p\n",
        "    JOIN\n",
        "        funcionario f ON p.funcPK = f.funcPK   -- Junção com a tabela de funcionários\n",
        "    JOIN\n",
        "        cargo c ON p.cargoPK = c.cargoPK       -- Junção com a tabela de cargos # Changed the join condition\n",
        "    JOIN\n",
        "        data d ON p.dataPK = d.dataPK          -- Junção com a tabela de datas\n",
        "    WHERE\n",
        "        d.dataAno = 2020                       -- Filtrar apenas o ano de 2020\n",
        "        AND c.cargoNivel = 'SENIOR'            -- Filtrar apenas cargos SENIOR\n",
        "    GROUP BY\n",
        "        f.funcEstadoNome, f.funcRegiaoNome     -- Agrupar por estado e região\n",
        "    ORDER BY\n",
        "        f.funcRegiaoNome, f.funcEstadoNome     -- Ordenar por região e estado\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar o resultado\n",
        "print(\"Consulta de salários por estado e região:\")\n",
        "consulta_salarios.show()\n"
      ],
      "metadata": {
        "id": "dK3Ln6g_iqR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fa4bd5-bf36-44d3-d0e7-8ab25b5615e6"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta de salários por estado e região:\n",
            "+--------------+--------+-------------+\n",
            "|        ESTADO|  REGIAO|TOTALSALARIOS|\n",
            "+--------------+--------+-------------+\n",
            "|    PERNAMBUCO|NORDESTE|    822273.25|\n",
            "|  MINAS GERAIS| SUDESTE|   2133676.23|\n",
            "|RIO DE JANEIRO| SUDESTE|   1094470.34|\n",
            "|     SAO PAULO| SUDESTE|   4623213.74|\n",
            "|        PARANA|     SUL|   2006090.29|\n",
            "+--------------+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados Obtidos:\n",
        "\n",
        "A consulta retornou os seguintes valores para os gastos totais com salários no ano de `2020`, considerando apenas funcionários com nível de cargo `SENIOR`:\n",
        "\n",
        "* PERNAMBUCO (NORDESTE): 822.273,25\n",
        "* MINAS GERAIS (SUDESTE): 1.333.766,23\n",
        "* RIO DE JANEIRO (SUDESTE): 1.049.476,34\n",
        "* SÃO PAULO (SUDESTE): 4.623.213,74\n",
        "* PARANÁ (SUL): 2.008.699,29"
      ],
      "metadata": {
        "id": "OfbBygRfAiz-"
      }
    }
  ]
}